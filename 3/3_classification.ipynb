{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z6ccN1AlFhNo"
   },
   "source": [
    "## Классификация текста с использованием FastText и CNN\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1U3vnZeD8aiDg5Gh-SjnEyJyfrTHSRTkB)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oJ8_zAI8FhNp"
   },
   "source": [
    "Используя данные отзывов IMDB, построим CNN для классификации документов на позитивный и негативный классы.\n",
    "\n",
    "Источник изложения: https://github.com/bentrevett/pytorch-sentiment-analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NHKLq9hEFhNr"
   },
   "source": [
    "В предположении, что PyTorch уже установлен, поставим дополнительные модули и загрузим модель для токенизации:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GgGxeuQjG9NI"
   },
   "outputs": [],
   "source": [
    "# !pip3 install https://download.pytorch.org/whl/cpu/torch-1.0.1.post2-cp36-cp36m-linux_x86_64.whl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5kiZro9eG-bG",
    "outputId": "7d5c6ae1-d4f6-44a4-8adc-a202b0a0118a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (0.10.0+cu102)\n",
      "Requirement already satisfied: torch==1.9.0 in /usr/local/lib/python3.7/dist-packages (from torchvision) (1.9.0+cu102)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchvision) (1.19.5)\n",
      "Requirement already satisfied: pillow>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision) (7.1.2)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.9.0->torchvision) (3.7.4.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a-WRgs-VFhNt",
    "outputId": "07620462-8715-40b8-d0dc-2950a6ee6663"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torchtext in /usr/local/lib/python3.7/dist-packages (0.10.0)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchtext) (1.19.5)\n",
      "Requirement already satisfied: torch==1.9.0 in /usr/local/lib/python3.7/dist-packages (from torchtext) (1.9.0+cu102)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from torchtext) (2.23.0)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from torchtext) (4.62.2)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.9.0->torchtext) (3.7.4.3)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext) (2.10)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext) (2021.5.30)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext) (1.24.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install torchtext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pB7uZ5L7FhN3"
   },
   "outputs": [],
   "source": [
    "# !pip3 install spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QLGeXcUjFhN8"
   },
   "outputs": [],
   "source": [
    "#!python3.6 -m spacy download en\n",
    "# !python3 -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QaPk16PLmgnw"
   },
   "outputs": [],
   "source": [
    "# import spacy\n",
    "#import en\n",
    "# en_nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FLGAJBxfFhN_"
   },
   "source": [
    "Загрузим датасет и получим из него выборку:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2zclld6vmaH2"
   },
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "s4Yq-LOGmoOn",
    "outputId": "0977ee5b-1081-4c8c-efc4-13ae6849572e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.9.0+cu102\n"
     ]
    }
   ],
   "source": [
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-ZlniJuAmp8F"
   },
   "outputs": [],
   "source": [
    "SEED = 0\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BObMLa5mvQYM"
   },
   "source": [
    "### Данные"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7w84GaXumr0q",
    "outputId": "b4571517-ed01-4552-f0b4-8e426b291742"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2021-10-02 14:57:44--  https://drive.google.com/uc?export=download&id=17uuANm7Q1CunXHfTaF7IRY9Vy7qPl5_L\n",
      "Распознаётся drive.google.com (drive.google.com)… 64.233.161.194\n",
      "Подключение к drive.google.com (drive.google.com)|64.233.161.194|:443... соединение установлено.\n",
      "HTTP-запрос отправлен. Ожидание ответа… 302 Moved Temporarily\n",
      "Адрес: https://doc-0c-44-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/2317m0m4j31ur7gl5fnbo8l4otc1u1as/1633175850000/13414369628864094336/*/17uuANm7Q1CunXHfTaF7IRY9Vy7qPl5_L?e=download [переход]\n",
      "Предупреждение: в HTTP маски не поддерживаются.\n",
      "--2021-10-02 14:58:08--  https://doc-0c-44-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/2317m0m4j31ur7gl5fnbo8l4otc1u1as/1633175850000/13414369628864094336/*/17uuANm7Q1CunXHfTaF7IRY9Vy7qPl5_L?e=download\n",
      "Распознаётся doc-0c-44-docs.googleusercontent.com (doc-0c-44-docs.googleusercontent.com)… 108.177.14.132\n",
      "Подключение к doc-0c-44-docs.googleusercontent.com (doc-0c-44-docs.googleusercontent.com)|108.177.14.132|:443... соединение установлено.\n",
      "HTTP-запрос отправлен. Ожидание ответа… 200 OK\n",
      "Длина: нет данных [text/csv]\n",
      "Сохранение в: «imdb.csv»\n",
      "\n",
      "imdb.csv                [            <=>     ]  63,14M  6,35MB/s    за 9,6s    \n",
      "\n",
      "2021-10-02 14:58:18 (6,55 MB/s) - «imdb.csv» сохранён [66212309]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget --no-check-certificate 'https://drive.google.com/uc?export=download&id=17uuANm7Q1CunXHfTaF7IRY9Vy7qPl5_L' -O imdb.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "pG25VDn1vdNn"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Basically there's a family where a little boy ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review sentiment\n",
       "0  One of the other reviewers has mentioned that ...  positive\n",
       "1  A wonderful little production. <br /><br />The...  positive\n",
       "2  I thought this was a wonderful way to spend ti...  positive\n",
       "3  Basically there's a family where a little boy ...  negative\n",
       "4  Petter Mattei's \"Love in the Time of Money\" is...  positive"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('imdb.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = train_test_split(df, test_size=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FastText\n",
    "\n",
    "Чтобы обучить FastText как классифкатор, необходимо сперва записать данные в файл в следующем формате:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "__label__0 some text\n",
    "__label__1 another text\n",
    "__label__0 the last text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fasttext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('ft_train_data.txt', 'w') as f:\n",
    "    for pair in list(zip(train['review'], train['sentiment'])):\n",
    "        text, label = pair\n",
    "        f.write(f'__label__{label} {text.lower()}\\n')\n",
    "        \n",
    "with open('ft_test_data.txt', 'w') as f:\n",
    "    for pair in list(zip(test['review'], test['sentiment'])):\n",
    "        text, label = pair\n",
    "        f.write(f'__label__{label} {text.lower()}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__label__positive got back from morocco then, where my dad was attached to the german embassy, when the film came out in europe; took all my girlfriends to see it, show them the beauty of the country, where jimi had played - and stayed -; where the hippies stopped after leaving ibiza and before joining goa. sean combs just celebrated at a friends restaurant in marrakech recently, the djema el fna was much wilder in those days than it is now; the stones went there earlier, brian recorded in tangiers ; so it's memories and family entertainment and i'm glad my son will get to know about the north of africa watching this movie. candice bergen looks beautiful and oo7/connery is pretty funny indeed; and teddy roosevelt as played by brian keith quite impressive.\r\n",
      "__label__negative having watched this movie several times, i have come to the conclusion that milos forman made a very daring decision to manufacture a muse for goya, when the artist led what most would consider a tempestuous,passionate life while the french revolution and the napoleonic era raged across europe, surely one that would have sufficient drama upon which to draw. while i do understand that mr. forman was relating in the microcosm of the tragedy of ines' life the devastation of the world at that time, i was left feeling that there was just so much of goya left out, so much of his humanity. the strongest and most eloquent point this film made was that because of man's fallen nature each of us is a potential villain in the stream of life, each of us has evil within us that we must fight with the help of god. how eloquent when goya says he should have helped ines more, how true for all of us! we must defend and protect the innocent. the superbly ironic scene in which the once imprisoned priest sentenced to die pronounces the death sentence on lorenzo who condemned him originally is the stuff of genius. i was left wanting something more when the credits rolled. maybe less of the unreal coincidences, and more of the inner life of the characters.\r\n",
      "__label__negative \"die sieger\" was highly recommended to be one of the few good action movies made in germany. i watched it last night and i must admit, that i am deeply disappointed. if that is supposed to be \"the last best hope\" for entertaining and challenging german action cinema, well then there is not much left.<br /><br />\"die sieger\" tries to be sexy, daring and furious but it is nothing of that kind. the characters are wooden and stereotype and whenever they do something unexpected (which doesn't happen too much) the act against their nature. that makes it hard - for me almost impossible - to follow them or even identify with them.<br /><br />most of all i think the film is very bad cast. there is not one character in whom i believe. maybe the superior officer at the swat unit - but that's about it. those people that try to look like or act like special units, like elite cops - i don't believe them. not for a second.<br /><br />the story is not so bad after all. but i think it's badly told. you don't get to know the bad guy at all - for example. and when after a \"very dark\" show down karl simon (the good guy) asks his already dead opponent \"why? ... what for?\" i did ask myself that very same question, knowing, that dominik graf wouldn't have the answer.<br /><br />i sincerely hope - no - i believe that germany can do better, even with action films.\r\n"
     ]
    }
   ],
   "source": [
    "! head -n 3 ft_train_data.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь можем обучить модель:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P@1: 0.8779333333333333\n",
      "R@1: 0.8779333333333333\n",
      "Number of examples: 15000\n"
     ]
    }
   ],
   "source": [
    "classifier = fasttext.train_supervised('ft_train_data.txt')#, 'model')\n",
    "result = classifier.test('ft_test_data.txt')\n",
    "print('P@1:', result[1])#.precision)\n",
    "print('R@1:', result[2])#.recall)\n",
    "print('Number of examples:', result[0])#.nexamples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Так можно получить сами предсказания модели:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['__label__negative'],\n",
       " ['__label__negative'],\n",
       " ['__label__positive'],\n",
       " ['__label__positive'],\n",
       " ['__label__negative'],\n",
       " ['__label__positive'],\n",
       " ['__label__negative'],\n",
       " ['__label__positive'],\n",
       " ['__label__negative'],\n",
       " ['__label__negative']]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred = classifier.predict(list(test['review']))[0]\n",
    "\n",
    "pred[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['negative',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'negative']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred = [label[0].split('__')[-1] for label in pred]\n",
    "\n",
    "pred[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3WAUUMcixIRP"
   },
   "source": [
    "## СNN\n",
    "\n",
    "### Записываем данные в удобные структуры"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yT8eYs4dxWW1"
   },
   "outputs": [],
   "source": [
    "from torchtext.legacy import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ck6B9K1ayX7A"
   },
   "outputs": [],
   "source": [
    "# классы Field и LabelField отвечают за то, как данные будут храниться и обрабатываться при считывании\n",
    "TEXT = data.Field(tokenize='spacy') # spacy -- значит, токенизацию будет делать модуль \n",
    "LABEL = data.LabelField()\n",
    "\n",
    "ds = data.TabularDataset(\n",
    "  path='imdb.csv', format='csv',\n",
    "  skip_header=True,\n",
    "  fields=[('text', TEXT),\n",
    "        ('label', LABEL)]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MdpfQN9qOu0p"
   },
   "source": [
    "ds - dataset - хранит итераторы по нашим данным и меткам класса.\n",
    "\n",
    "**NB**: неважно как столбцы назывались в исходном датасете, здесь за названия отвечают строки, которые мы передали аргументу `fields`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "O8-CSOf1xHuV",
    "outputId": "0e4a34a7-f850-411e-80bf-4a0c18dd00af"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['One',\n",
       " 'of',\n",
       " 'the',\n",
       " 'other',\n",
       " 'reviewers',\n",
       " 'has',\n",
       " 'mentioned',\n",
       " 'that',\n",
       " 'after',\n",
       " 'watching']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(ds.text)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "e9h0nnsi0Q0-",
    "outputId": "a04507ff-84e2-4306-d9fa-d00fea14b077"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'positive'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(ds.label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ECxpxH2m9VY8"
   },
   "source": [
    "Строим словарь и загружаем эмбеддинги.\n",
    "\n",
    "С учётом того, что в коллекции 100К уникальных слов, и векторы получатся достаточно громоздкие, урежем коллекцию до 25К слов, для всех прочих заведя токен unk (unknown).\n",
    "\n",
    "У torchtext есть репозиторий, где хранятся некоторые словарные эмбеддинги для английского. `vectors=\"glove.6B.100d\"` значит, что крооме построения индекса слов корпуса, мы скачаем и сохраним вектора glove из этого репозитория."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9K_LTwEn78n5",
    "outputId": "d2283043-292b-494e-bf01-b2e0ae02fcff"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ".vector_cache/glove.6B.zip: 862MB [02:40, 5.36MB/s]                           \n",
      "100%|█████████▉| 399999/400000 [00:14<00:00, 27750.91it/s]\n"
     ]
    }
   ],
   "source": [
    "TEXT.build_vocab(ds, max_size=25000, vectors=\"glove.6B.100d\")\n",
    "LABEL.build_vocab(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d9omo_uS9P7V",
    "outputId": "c70f3a41-40a3-4184-d0b0-81f093a7596e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<unk>', '<pad>', 'the', ',', '.', 'a', 'and', 'of', 'to', 'is', 'in', 'I', 'it', 'that', '\"', \"'s\", 'this', '-', '/><br', 'was']\n"
     ]
    }
   ],
   "source": [
    "# itos == i to s == index to string\n",
    "print(TEXT.vocab.itos[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eNFCBfJZa98r",
    "outputId": "e7c0cfd3-d4a8-4bdd-cb88-206e1e51dd00"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<unk>',\n",
       " '<pad>',\n",
       " 'the',\n",
       " ',',\n",
       " '.',\n",
       " 'a',\n",
       " 'and',\n",
       " 'of',\n",
       " 'to',\n",
       " 'is',\n",
       " 'in',\n",
       " 'I',\n",
       " 'it',\n",
       " 'that',\n",
       " '\"',\n",
       " \"'s\",\n",
       " 'this',\n",
       " '-',\n",
       " '/><br',\n",
       " 'was']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TEXT.vocab.itos[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eZRu4ykFpz_q",
    "outputId": "963f34e4-00e4-4548-cbc3-4389451b1b00"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# stoi == s to i == string to index\n",
    "TEXT.vocab.stoi[42]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AJl0ZOPBElSE"
   },
   "source": [
    "Разобьём обучающий сет на обучение, валидацию для настройки параметров и тест."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6nuZQIq1FhOl"
   },
   "outputs": [],
   "source": [
    "train, val = ds.split() # дефолтное соотношение 0.7\n",
    "val, test = val.split(split_ratio=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d4_u1nkyS740",
    "outputId": "9d484da5-a4a3-4ac6-ffb2-99be77d16441"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35000\n",
      "7500\n",
      "7500\n"
     ]
    }
   ],
   "source": [
    "print(len(train))\n",
    "print(len(val))\n",
    "print(len(test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eT-lY0stTbWo"
   },
   "source": [
    "А теперь создадим итераторы батчей:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lIJTX3ypTbrK"
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE  = 64\n",
    "\n",
    "train_iterator, valid_iterator, test_iterator = data.BucketIterator.splits(\n",
    "    (train, val, test), \n",
    "    batch_size=BATCH_SIZE, \n",
    "    sort=True,\n",
    "    sort_key=lambda x: len(x.text), # сорируем тексты по длине, чтобы рядом оказывались предложения с одинаковой длиной и добавлялось меньше паддинга\n",
    "    repeat=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2JtvOuVbT3i7"
   },
   "source": [
    "Заглянем внутрь батча"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UQuNMEB5Tgmz",
    "outputId": "44e81c65-4221-41bf-c790-069233a67ea5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64\n",
      "64\n",
      "64\n",
      "64\n",
      "64\n",
      "64\n",
      "64\n",
      "64\n",
      "64\n",
      "64\n",
      "64\n",
      "64\n",
      "64\n",
      "64\n",
      "64\n",
      "64\n",
      "64\n",
      "64\n",
      "64\n",
      "64\n",
      "64\n",
      "64\n",
      "64\n",
      "64\n",
      "64\n",
      "64\n",
      "64\n",
      "64\n",
      "64\n",
      "64\n",
      "64\n",
      "64\n",
      "64\n",
      "64\n",
      "64\n",
      "64\n",
      "64\n",
      "64\n",
      "64\n",
      "64\n",
      "64\n",
      "64\n",
      "64\n",
      "64\n",
      "64\n",
      "64\n",
      "64\n",
      "64\n",
      "64\n",
      "64\n",
      "64\n",
      "64\n",
      "64\n",
      "64\n",
      "64\n",
      "64\n",
      "64\n",
      "64\n",
      "64\n",
      "64\n",
      "64\n",
      "64\n",
      "64\n",
      "64\n",
      "64\n",
      "64\n",
      "64\n",
      "64\n",
      "64\n",
      "64\n",
      "64\n",
      "64\n",
      "64\n",
      "64\n",
      "64\n",
      "64\n",
      "64\n",
      "64\n",
      "64\n",
      "64\n",
      "64\n",
      "64\n",
      "64\n",
      "64\n",
      "64\n",
      "64\n",
      "64\n",
      "64\n",
      "64\n",
      "64\n",
      "64\n",
      "64\n",
      "64\n",
      "64\n",
      "64\n",
      "64\n",
      "64\n",
      "64\n",
      "64\n",
      "64\n",
      "64\n",
      "64\n",
      "64\n",
      "64\n",
      "64\n",
      "64\n",
      "64\n",
      "64\n",
      "64\n",
      "64\n",
      "64\n",
      "64\n",
      "64\n",
      "64\n",
      "64\n",
      "64\n",
      "64\n",
      "12\n"
     ]
    }
   ],
   "source": [
    "for i, batch in enumerate(test_iterator):\n",
    "  print(batch.batch_size)\n",
    "  # pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_kE9EDfGT_Gr",
    "outputId": "a3716a38-5a4a-4c11-b434-480392cb5e53"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['text', 'label'])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch.fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9Kvf2Hf2UEtR",
    "outputId": "b5411668-fa08-46f7-8bb2-300bcef2b728"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch.batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "l1oLPN8-TuA_",
    "outputId": "640b5ed0-8d30-4f78-8dfb-77e9c3f54c64"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  149,   455,    50,  ...,    11,  2823,   399],\n",
       "        [  149,     0,    25,  ...,   461,     0, 23467],\n",
       "        [  149,     0,  9509,  ...,  3791,   108,    23],\n",
       "        ...,\n",
       "        [    2,     1,     1,  ...,     1,     1,     1],\n",
       "        [ 5352,     1,     1,  ...,     1,     1,     1],\n",
       "        [    4,     1,     1,  ...,     1,     1,     1]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8_TjGTb0UHDE",
    "outputId": "a3c1fb28-d6be-4c9c-992e-7542000ffd8c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch.label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y7i8JbEnULFC"
   },
   "source": [
    "## Обучение"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RTPx1IJ2SlXf"
   },
   "source": [
    "### Модель"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jRr8D0t3Sl1a"
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UHEDOjrAFhP5"
   },
   "source": [
    "Для создания свёрточного слоя воспользуемся nn.Conv2d, in_channels в нашем случае один (текст), out_channels -- это число число фильтров и размер ядер всех фильтров. Каждый фильтр будет иметь размерность [n x размерность эмбеддинга], где n - размер обрабатываемой n-граммы.\n",
    "\n",
    "Важно, что предложения имели длину не меньше размера самого большого из используемых фильтров (здесь это не страшно, поскольку в используемых данных нет текстов, состоящих из пяти и менее слов)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yb5g3czaFhP6"
   },
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, n_filters, filter_sizes, output_dim, dropout_proba):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.conv_0 = nn.Conv2d(in_channels=1, out_channels=n_filters, kernel_size=(filter_sizes[0], embedding_dim))\n",
    "        self.conv_1 = nn.Conv2d(in_channels=1, out_channels=n_filters, kernel_size=(filter_sizes[1], embedding_dim))\n",
    "        self.conv_2 = nn.Conv2d(in_channels=1, out_channels=n_filters, kernel_size=(filter_sizes[2], embedding_dim))\n",
    "        self.fc = nn.Linear(len(filter_sizes) * n_filters, output_dim)\n",
    "        self.dropout = nn.Dropout(dropout_proba)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        #x = [sent len, batch size]\n",
    "        x = x.permute(1, 0)\n",
    "                \n",
    "        #x = [batch size, sent len]\n",
    "        embedded = self.embedding(x)\n",
    "        #print(embedded.shape)\n",
    "                \n",
    "        #embedded = [batch size, sent len, emb dim]\n",
    "        embedded = embedded.unsqueeze(1)\n",
    "        \n",
    "        #embedded = [batch size, 1, sent len, emb dim]\n",
    "        conv_0 = self.conv_0(embedded)\n",
    "        #print(conv_0.shape)\n",
    "        conv_0 = conv_0.squeeze(3)\n",
    "        #print(conv_0.shape)\n",
    "        conved_0 = F.relu(conv_0)\n",
    "        conved_1 = F.relu(self.conv_1(embedded).squeeze(3))\n",
    "        conved_2 = F.relu(self.conv_2(embedded).squeeze(3))\n",
    "            \n",
    "        #conv_n = [batch size, n_filters, sent len - filter_sizes[n]]\n",
    "        #print(conved_0.shape)\n",
    "        pool_0 = F.max_pool1d(conved_0, conved_0.shape[2])\n",
    "        #print(pool_0.shape)\n",
    "\n",
    "        pooled_0 = pool_0.squeeze(2)\n",
    "        #print(pooled_0.shape)\n",
    "        pooled_1 = F.max_pool1d(conved_1, conved_1.shape[2]).squeeze(2)\n",
    "        pooled_2 = F.max_pool1d(conved_2, conved_2.shape[2]).squeeze(2)\n",
    "        \n",
    "        #pooled_n = [batch size, n_filters]\n",
    "        cat = self.dropout(torch.cat((pooled_0, pooled_1, pooled_2), dim=1))\n",
    "\n",
    "        #cat = [batch size, n_filters * len(filter_sizes)]\n",
    "        return self.fc(cat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e_xPva9zFhP8"
   },
   "source": [
    "Сейчас мы можем использовать только три различных фильтра, хотелось бы больше. Вообще, можно воспользоваться `nn.ModuleList`, чтобы создать слои списком и сделать так, чтобы фильтров создавалось по количеству элементов в filter_sizes. [(Как здесь).](https://github.com/bentrevett/pytorch-sentiment-analysis/blob/master/4%20-%20Convolutional%20Sentiment%20Analysis.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FenASHjKUb3p"
   },
   "source": [
    "### Вспомогательные функции"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yf2viKyOE5FF"
   },
   "source": [
    "Опишем функцию подсчёта accuracy, а также функции обучения и применения сети:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VQo4PuPzFhPE"
   },
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "def binary_accuracy(preds, y):\n",
    "    rounded_preds = torch.round(F.sigmoid(preds))\n",
    "    correct = (rounded_preds == y).float()\n",
    "    acc = correct.sum() / len(correct)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jepSRNlkFhPI"
   },
   "outputs": [],
   "source": [
    "def train_func(model, iterator, optimizer, criterion):\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    for batch in iterator:\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        predictions = model(batch.text.cuda()).squeeze(1)\n",
    "\n",
    "        loss = criterion(predictions.float(), batch.label.float().cuda())\n",
    "        acc = binary_accuracy(predictions.float(), batch.label.float().cuda())\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss\n",
    "        epoch_acc += acc\n",
    "        \n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nb0KPBl8FhPL"
   },
   "outputs": [],
   "source": [
    "def evaluate_func(model, iterator, criterion):\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in iterator:\n",
    "            predictions = model(batch.text.cuda()).squeeze(1)\n",
    "\n",
    "            loss = criterion(predictions.float(), batch.label.float().cuda())\n",
    "            acc = binary_accuracy(predictions.float(), batch.label.float().cuda())\n",
    "\n",
    "            epoch_loss += loss\n",
    "            epoch_acc += acc\n",
    "        \n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T6CzZJecUpN7"
   },
   "source": [
    "### Подготовка обучения"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4Q_Yvs_gFhQB"
   },
   "outputs": [],
   "source": [
    "INPUT_DIM = len(TEXT.vocab)\n",
    "EMBEDDING_DIM = 100\n",
    "N_FILTERS = 100\n",
    "FILTER_SIZES = [3,4,5]\n",
    "OUTPUT_DIM = 1\n",
    "DROPOUT_PROBA = 0.5\n",
    "\n",
    "model = CNN(INPUT_DIM, EMBEDDING_DIM, N_FILTERS, FILTER_SIZES, OUTPUT_DIM, DROPOUT_PROBA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8-5mPhI4U0nk",
    "outputId": "e8130166-723b-4545-b8a3-4b846c449f3c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CNN(\n",
       "  (embedding): Embedding(25002, 100)\n",
       "  (conv_0): Conv2d(1, 100, kernel_size=(3, 100), stride=(1, 1))\n",
       "  (conv_1): Conv2d(1, 100, kernel_size=(4, 100), stride=(1, 1))\n",
       "  (conv_2): Conv2d(1, 100, kernel_size=(5, 100), stride=(1, 1))\n",
       "  (fc): Linear(in_features=300, out_features=1, bias=True)\n",
       "  (dropout): Dropout(p=0.5, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model # посмотрим на неё ещё раз"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lVfQU3wUU4BY"
   },
   "source": [
    "Копируем скачанные эмбеддинги слов в параметры слоя `Embedding`, чботы не нужно было обучать его с нуля."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "18Vk11CaZG-u",
    "outputId": "9e0a3765-91bb-4253-d784-33c77609235b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "        [-0.0382, -0.2449,  0.7281,  ..., -0.1459,  0.8278,  0.2706],\n",
       "        ...,\n",
       "        [ 0.4413,  0.3325,  0.1120,  ..., -0.0686,  0.4374,  0.8717],\n",
       "        [ 0.1177,  0.1141,  0.2218,  ..., -1.0694,  0.4712, -0.7554],\n",
       "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pretrained_embeddings = TEXT.vocab.vectors\n",
    "model.embedding.weight.data.copy_(pretrained_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1Yqwwc5rBc5L"
   },
   "outputs": [],
   "source": [
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "np-BTnydFhQF"
   },
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters()) # мы подали оптимизатору все параметры -- значит, эмбеддиги тоже будут дообучаться\n",
    "criterion = nn.BCEWithLogitsLoss() # бинарная кросс-энтропия с логитами\n",
    "\n",
    "model = model.cuda() # будем учить на gpu! =)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AmWIaqIbFhQF"
   },
   "source": [
    "### Обучение!\n",
    "\n",
    "Используя определённые ранее функции, запустим обучение с оптимизатором Adam и оценим качество на валидации и тесте:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XZC7S33pFhQH",
    "outputId": "3e689afa-3d0c-4a51-cc0b-fee0303dfad2"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:652: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1156.)\n",
      "  return torch.max_pool1d(input, kernel_size, stride, padding, dilation, ceil_mode)\n",
      "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1805: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01, Train Loss: 0.406, Train Acc: 81.15%, Val. Loss: 0.298, Val. Acc: 87.31%\n",
      "Epoch: 02, Train Loss: 0.252, Train Acc: 89.65%, Val. Loss: 0.253, Val. Acc: 89.87%\n",
      "Epoch: 03, Train Loss: 0.180, Train Acc: 93.05%, Val. Loss: 0.239, Val. Acc: 90.65%\n"
     ]
    }
   ],
   "source": [
    "N_EPOCHS = 5\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "    train_loss, train_acc = train_func(model, train_iterator, optimizer, criterion)\n",
    "    valid_loss, valid_acc = evaluate_func(model, valid_iterator, criterion)\n",
    "    \n",
    "    print(f'Epoch: {epoch+1:02}, Train Loss: {train_loss:.3f}, Train Acc: {train_acc*100:.2f}%, Val. Loss: {valid_loss:.3f}, Val. Acc: {valid_acc*100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bXQYQCLCFhQJ",
    "outputId": "98efa95a-536a-4753-feb0-c36819a41d87"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1805: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.282, Test Acc: 88.24%\n"
     ]
    }
   ],
   "source": [
    "test_loss , test_acc = evaluate_func(model, test_iterator, criterion)\n",
    "print(f'Test Loss: {test_loss:.3f}, Test Acc: {test_acc*100:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Mm5CmdjtY6Zj"
   },
   "source": [
    "#### Упражнение 1: как изменились эмбеддинги?\n",
    "\n",
    "Давайте проверим, произошли ли какие-то любопытные изменения в отношениях между словами."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xbAon9WMY61-",
    "outputId": "05023a82-40b3-4cd9-b5e0-594ed183cac8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "        [-0.0382, -0.2449,  0.7281,  ..., -0.1459,  0.8278,  0.2706],\n",
       "        ...,\n",
       "        [ 0.4413,  0.3325,  0.1120,  ..., -0.0686,  0.4374,  0.8717],\n",
       "        [ 0.1177,  0.1141,  0.2218,  ..., -1.0694,  0.4712, -0.7554],\n",
       "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TEXT.vocab.vectors # старые эмбеддинги"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Oej9zvPYarQK",
    "outputId": "fdca1239-3c5d-4829-9a5e-3f623701a060"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.3537, -0.5994, -0.2251,  ...,  0.0064, -1.3333,  0.3274],\n",
       "        [-0.2687, -0.0785, -1.9346,  ..., -1.7024, -0.1569,  0.8511],\n",
       "        [ 0.4343, -0.0671, -1.0944,  ..., -0.3915, -0.9669,  0.3443],\n",
       "        ...,\n",
       "        [-0.5355,  1.5282,  0.3985,  ...,  0.6649, -1.7519, -1.8649],\n",
       "        [ 0.2111,  1.9653,  1.2217,  ..., -1.6369,  0.9221,  0.5238],\n",
       "        [-1.5200,  0.1436,  1.1655,  ...,  0.0534,  0.7600, -1.1566]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.embedding.weight.data # новые эмбеддиги"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "k8mHfM5MbhSx"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qcFo6zIfbVvL"
   },
   "outputs": [],
   "source": [
    "i1, i2 = TEXT.vocab.stoi['perfect'], TEXT.vocab.stoi['awful']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "17bFXBwhbliQ",
    "outputId": "51531905-18f3-4c5a-8725-9b0f395ae502"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.9999999, 0.5248411],\n",
       "       [0.5248411, 0.9999996]], dtype=float32)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_similarity([\n",
    "  TEXT.vocab.vectors[i1].cpu().numpy(),\n",
    "  TEXT.vocab.vectors[i2].cpu().numpy()\n",
    "  ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YUFaea6bbw8f",
    "outputId": "9d89db00-a133-403b-8c9a-330626f9ffd6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.9999999, -0.086183 ],\n",
       "       [-0.086183 ,  0.9999996]], dtype=float32)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_similarity([\n",
    "  model.embedding.weight.data[i1].cpu().numpy(),\n",
    "  model.embedding.weight.data[i2].cpu().numpy()\n",
    "  ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u0XE_D_MWE22"
   },
   "source": [
    "\"прекрасный\" и \"ужасный\" стали дальше друг от друга\n",
    "\n",
    "**Задание**: посмотрите на другие изменения и попробуйте их объяснить. Для наглядности можно сделать визуализацию с помощью t-sne."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aVvpv9w-WFOc"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ICox-i-WYYPq"
   },
   "source": [
    "#### Упражнение 2: nn.ModuleList\n",
    "\n",
    "Используя его, можно легко и красиво определить столько разных сверток, сколько захотим! Вот пример:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eKCFgWKdXeyO"
   },
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, n_filters, filter_sizes, output_dim, \n",
    "                 dropout, pad_idx):\n",
    "        \n",
    "        super().__init__()\n",
    "                \n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx = pad_idx)\n",
    "        \n",
    "        self.convs = nn.ModuleList([\n",
    "                                    nn.Conv2d(in_channels = 1, \n",
    "                                              out_channels = n_filters, \n",
    "                                              kernel_size = (fs, embedding_dim)) \n",
    "                                    for fs in filter_sizes\n",
    "                                    ])\n",
    "        \n",
    "        self.fc = nn.Linear(len(filter_sizes) * n_filters, output_dim)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, text):\n",
    "                \n",
    "        #text = [batch size, sent len]\n",
    "        \n",
    "        embedded = self.embedding(text)\n",
    "                \n",
    "        #embedded = [batch size, sent len, emb dim]\n",
    "        \n",
    "        embedded = embedded.unsqueeze(1)\n",
    "        \n",
    "        #embedded = [batch size, 1, sent len, emb dim]\n",
    "        \n",
    "        conved = [F.relu(conv(embedded)).squeeze(3) for conv in self.convs]\n",
    "            \n",
    "        #conved_n = [batch size, n_filters, sent len - filter_sizes[n] + 1]\n",
    "                \n",
    "        pooled = [F.max_pool1d(conv, conv.shape[2]).squeeze(2) for conv in conved]\n",
    "        \n",
    "        #pooled_n = [batch size, n_filters]\n",
    "        \n",
    "        cat = self.dropout(torch.cat(pooled, dim = 1))\n",
    "\n",
    "        #cat = [batch size, n_filters * len(filter_sizes)]\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MSb06TT3cwO9"
   },
   "source": [
    "**Задание**: поэкспериментируйте с количеством и размером сверток. Что сработает лучше?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eO5TShOrc5OV"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GGNUN9TTYnxO"
   },
   "source": [
    "#### Упражнение 3: другая предобработка\n",
    "\n",
    "При загрузке данных, мы использовали `data.Field(tokenize='spacy')`.\n",
    "Попробуем заменить токенизатор `spacy` на свою функцию, которая дополнительно чистит данные от мусора."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 272
    },
    "id": "YkmL4MxZZ1N0",
    "outputId": "ff2fdf12-d892-451c-e5ea-c52b626308bd"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['is',\n",
       " 'exactly',\n",
       " 'what',\n",
       " 'happened',\n",
       " 'with',\n",
       " 'me.<br',\n",
       " '/><br',\n",
       " '/>The',\n",
       " 'first',\n",
       " 'thing',\n",
       " 'that',\n",
       " 'struck',\n",
       " 'me',\n",
       " 'about',\n",
       " 'Oz']"
      ]
     },
     "execution_count": 71,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# пример мусора\n",
    "ds.examples[0].text[25:40]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yRV729TtZfxC"
   },
   "source": [
    "Предобработка (из прошлого семинара):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bnDJ5sFwZfxD"
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iL2EXExRZfxF"
   },
   "outputs": [],
   "source": [
    "def review_to_wordlist(review):\n",
    "    # убираем ссылки\n",
    "    review = re.sub(r\"http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+\", \" \", review)\n",
    "    # достаем сам текст\n",
    "    review_text = BeautifulSoup(review, \"lxml\").get_text()\n",
    "    # оставляем только буквенные символы\n",
    "    review_text = re.sub(\"[^a-zA-Z]\",\" \", review_text)\n",
    "    # приводим к нижнему регистру и разбиваем на слова по символу пробела\n",
    "    return review_text.lower().split() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xXbasRI5dBfM"
   },
   "source": [
    "**Задание**: попробуйте обучить модель, используя другую предобработку. Стало ли лучше? Что если убирать стоп-слова?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "L4PiG-hIZfxH"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Sfuamkw28rgp"
   },
   "source": [
    "# Аугментация данных"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "89zDRG-jIdNB"
   },
   "source": [
    "В нашем примере данные были сбалансированными, а как работать с небалансированными данными?\n",
    "\n",
    "Рассмотрим задачу распознавания тональности твитов, взятых из [Twitter Sentimental Analysis challenge](https://datahack.analyticsvidhya.com/contest/practice-problem-twitter-sentiment-analysis/).\n",
    "\n",
    "Источник изложения: https://github.com/mabusalah/Resampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uLKzWFu-QWAm"
   },
   "source": [
    "Получим данные"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5_WpbqWhk-a5",
    "outputId": "05aac62c-531b-406a-8b84-9fd42c26a29e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2021-09-23 14:47:37--  https://drive.google.com/uc?export=download&id=1Jjuk23nMTQkfA3-3_HpevXGeupav7QLz\n",
      "Resolving drive.google.com (drive.google.com)... 142.251.2.139, 142.251.2.102, 142.251.2.113, ...\n",
      "Connecting to drive.google.com (drive.google.com)|142.251.2.139|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Moved Temporarily\n",
      "Location: https://doc-0s-44-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/7u8rheh5adhdiu2fhv5a3r2n5r6siiss/1632408450000/13414369628864094336/*/1Jjuk23nMTQkfA3-3_HpevXGeupav7QLz?e=download [following]\n",
      "Warning: wildcards not supported in HTTP.\n",
      "--2021-09-23 14:47:38--  https://doc-0s-44-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/7u8rheh5adhdiu2fhv5a3r2n5r6siiss/1632408450000/13414369628864094336/*/1Jjuk23nMTQkfA3-3_HpevXGeupav7QLz?e=download\n",
      "Resolving doc-0s-44-docs.googleusercontent.com (doc-0s-44-docs.googleusercontent.com)... 142.251.2.132, 2607:f8b0:4023:c0d::84\n",
      "Connecting to doc-0s-44-docs.googleusercontent.com (doc-0s-44-docs.googleusercontent.com)|142.251.2.132|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: unspecified [text/csv]\n",
      "Saving to: ‘train.csv’\n",
      "\n",
      "train.csv               [ <=>                ]   2.96M  15.8MB/s    in 0.2s    \n",
      "\n",
      "2021-09-23 14:47:39 (15.8 MB/s) - ‘train.csv’ saved [3103165]\n",
      "\n",
      "--2021-09-23 14:47:39--  https://drive.google.com/uc?export=download&id=11FugxTRrdKqkDE_3KlfCDWRn_rbR6VxM\n",
      "Resolving drive.google.com (drive.google.com)... 142.251.2.139, 142.251.2.113, 142.251.2.100, ...\n",
      "Connecting to drive.google.com (drive.google.com)|142.251.2.139|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Moved Temporarily\n",
      "Location: https://doc-04-44-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/56lg2mialmqm3htsmr0ru5jd3qbqb4pr/1632408450000/13414369628864094336/*/11FugxTRrdKqkDE_3KlfCDWRn_rbR6VxM?e=download [following]\n",
      "Warning: wildcards not supported in HTTP.\n",
      "--2021-09-23 14:47:40--  https://doc-04-44-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/56lg2mialmqm3htsmr0ru5jd3qbqb4pr/1632408450000/13414369628864094336/*/11FugxTRrdKqkDE_3KlfCDWRn_rbR6VxM?e=download\n",
      "Resolving doc-04-44-docs.googleusercontent.com (doc-04-44-docs.googleusercontent.com)... 142.251.2.132, 2607:f8b0:4023:c0d::84\n",
      "Connecting to doc-04-44-docs.googleusercontent.com (doc-04-44-docs.googleusercontent.com)|142.251.2.132|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 1635543 (1.6M) [text/csv]\n",
      "Saving to: ‘test.csv’\n",
      "\n",
      "test.csv            100%[===================>]   1.56M  7.83MB/s    in 0.2s    \n",
      "\n",
      "2021-09-23 14:47:41 (7.83 MB/s) - ‘test.csv’ saved [1635543/1635543]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget --no-check-certificate \"https://drive.google.com/uc?export=download&id=1Jjuk23nMTQkfA3-3_HpevXGeupav7QLz\" -O train.csv\n",
    "!wget --no-check-certificate \"https://drive.google.com/uc?export=download&id=11FugxTRrdKqkDE_3KlfCDWRn_rbR6VxM\" -O test.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ILRsSOuWJGFP",
    "outputId": "542da2b1-6ee5-4c9a-b48b-ffcef8d05f73"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Set: (17197, 2) 17197\n",
      "Training Set: (31962, 3) 31962\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "test = pd.read_csv('test.csv')\n",
    "print(\"Test Set:\"% test.columns, test.shape, len(test))\n",
    "train = pd.read_csv('train.csv')\n",
    "print(\"Training Set:\"% train.columns, train.shape, len(train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 203
    },
    "id": "sBtAsQDNJPXX",
    "outputId": "45bbcd2a-d511-455d-c96e-c404e6337a38"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>label</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>@user when a father is dysfunctional and is s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>@user @user thanks for #lyft credit i can't us...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>bihday your majesty</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>#model   i love u take with u all the time in ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>factsguide: society now    #motivation</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  label                                              tweet\n",
       "0   1      0   @user when a father is dysfunctional and is s...\n",
       "1   2      0  @user @user thanks for #lyft credit i can't us...\n",
       "2   3      0                                bihday your majesty\n",
       "3   4      0  #model   i love u take with u all the time in ...\n",
       "4   5      0             factsguide: society now    #motivation"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 203
    },
    "id": "XV-4lGQVJRc2",
    "outputId": "4bdbc8db-e250-4c9d-a592-b017e95c7fef"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>31963</td>\n",
       "      <td>#studiolife #aislife #requires #passion #dedic...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>31964</td>\n",
       "      <td>@user #white #supremacists want everyone to s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>31965</td>\n",
       "      <td>safe ways to heal your #acne!!    #altwaystohe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>31966</td>\n",
       "      <td>is the hp and the cursed child book up for res...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>31967</td>\n",
       "      <td>3rd #bihday to my amazing, hilarious #nephew...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id                                              tweet\n",
       "0  31963  #studiolife #aislife #requires #passion #dedic...\n",
       "1  31964   @user #white #supremacists want everyone to s...\n",
       "2  31965  safe ways to heal your #acne!!    #altwaystohe...\n",
       "3  31966  is the hp and the cursed child book up for res...\n",
       "4  31967    3rd #bihday to my amazing, hilarious #nephew..."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xarXsO4DJU8Y"
   },
   "source": [
    "Итак, посмотрим, какой процент от общей выборки занимают позитивные и негативные примеры."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eT7Rz3MNJTVP",
    "outputId": "5b4c657d-4435-4015-c586-fcf3444684e2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive:  92.98542018647143 %\n",
      "Negative:  7.014579813528565 %\n"
     ]
    }
   ],
   "source": [
    "print(\"Positive: \", train.label.value_counts()[0]/len(train)*100,\"%\")\n",
    "print(\"Negative: \", train.label.value_counts()[1]/len(train)*100,\"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VB6Jaz-mJ1zR"
   },
   "source": [
    "93% vs. 7% - данные определенно несбалансированны, что, в свою очередь, негативно влияет на точность предсказания.\n",
    "Для начала поработаем с исходными данными и оценим точность классификации.\n",
    "Начнем с предобработки данных: уберем из твитов числа, html/xml-тэги, специальные символы."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FCmZHO1kJfpP"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "from bs4 import BeautifulSoup #для работы с html/xml-тэгами\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "porter=PorterStemmer()\n",
    "tok = WordPunctTokenizer()\n",
    "pat1 = r'@[A-Za-z0-9]+'\n",
    "pat2 = r'https?://[A-Za-z0-9./]+'\n",
    "combined_pat = r'|'.join((pat1, pat2))\n",
    "\n",
    "def tweet_cleaner(text):\n",
    "    soup = BeautifulSoup(text, 'lxml')\n",
    "    souped = soup.get_text()\n",
    "    stripped = re.sub(combined_pat, '', souped)\n",
    "    try:\n",
    "        clean = stripped.decode(\"utf-8-sig\").replace(u\"\\ufffd\", \"?\")\n",
    "    except:\n",
    "        clean = stripped\n",
    "    letters_only = re.sub(\"[^a-zA-Z]\", \" \", clean)\n",
    "    lower_case = letters_only.lower()\n",
    "\n",
    "    words = tok.tokenize(lower_case)\n",
    "    \n",
    "    stem_sentence=[]\n",
    "    for word in words:\n",
    "        stem_sentence.append(porter.stem(word))\n",
    "        stem_sentence.append(\" \")\n",
    "    words=\"\".join(stem_sentence).strip()\n",
    "    return words\n",
    "\n",
    "nums = [0,len(train)]\n",
    "clean_tweet_texts = []\n",
    "for i in range(nums[0],nums[1]):\n",
    "    clean_tweet_texts.append(tweet_cleaner(train['tweet'][i]))\n",
    "    \n",
    "nums = [0,len(test)]\n",
    "test_tweet_texts = []\n",
    "\n",
    "for i in range(nums[0],nums[1]):\n",
    "    test_tweet_texts.append(tweet_cleaner(test['tweet'][i])) \n",
    "    \n",
    "train_clean = pd.DataFrame(clean_tweet_texts,columns=['tweet'])\n",
    "train_clean['label'] = train.label\n",
    "train_clean['id'] = train.id\n",
    "test_clean = pd.DataFrame(test_tweet_texts,columns=['tweet'])\n",
    "test_clean['id'] = test.id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lkQ5fDVUM9EU"
   },
   "source": [
    "Разделим данные на обучающие и проверочные."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d23SW3-tMdKk"
   },
   "outputs": [],
   "source": [
    "from sklearn import model_selection, preprocessing, metrics, linear_model, svm\n",
    "\n",
    "train_x, valid_x, train_y, valid_y = model_selection.train_test_split(train_clean['tweet'],train_clean['label'])\n",
    "encoder = preprocessing.LabelEncoder()\n",
    "train_y = encoder.fit_transform(train_y)\n",
    "valid_y = encoder.fit_transform(valid_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Tfj1ZjQANVSK"
   },
   "source": [
    "Рассчитаем TF-IDF признаки."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RNw9U1WnNKfq"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "\n",
    "tfidf_vect = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', max_features=100000)\n",
    "tfidf_vect.fit(train_clean['tweet'])\n",
    "xtrain_tfidf =  tfidf_vect.transform(train_x)\n",
    "xvalid_tfidf =  tfidf_vect.transform(valid_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K8TxxultNlOq"
   },
   "source": [
    "Точность в качестве метрики работает хорошо только на сбалансированных наборах данных, поэтому для оценки результатов работы  алгоритма будем использовать F1-метрику."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cx29ZstgNXqb"
   },
   "outputs": [],
   "source": [
    "def train_model(classifier, feature_vector_train, label, feature_vector_valid):\n",
    "    classifier.fit(feature_vector_train, label)\n",
    "\n",
    "    predictions = classifier.predict(feature_vector_valid)    \n",
    "\n",
    "    return metrics.f1_score(valid_y,predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_YEjiEUDNrLj"
   },
   "source": [
    "Для начала обучим лог-регрессию."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BG3DifE-Nn-I",
    "outputId": "3105c05d-8737-447d-b758-1c2ac59a6530"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic regression Baseline, WordLevel TFIDF:  0.5401459854014599\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    }
   ],
   "source": [
    "accuracyORIGINAL = train_model(linear_model.LogisticRegression(random_state=0, solver='lbfgs',multi_class='multinomial'),xtrain_tfidf, train_y, xvalid_tfidf)\n",
    "print (\"Logistic regression Baseline, WordLevel TFIDF: \", accuracyORIGINAL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rIaiuRviW5pS"
   },
   "source": [
    "Попробуйте использовать обычный счетчик слов для извлечения признаков."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Vsr0wT4NW2LW"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X0baBnaYN7a0"
   },
   "source": [
    "Как видно, результат оставляет желать лучшего.\n",
    "\n",
    "Что можно сделать с данными?\n",
    "\n",
    "Было бы неплохо как-то увеличить  количество негативных примеров, или же уменьшить количество положительных. Для этого существуют различные техники аугментации данных. \n",
    "В Python для этих целей есть библиотека imblearn (imbalanced-learn)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "F6zNqmnUN1WB",
    "outputId": "d2a9ec59-cd71-4727-ae69-22dc283ef5cf"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/externals/six.py:31: FutureWarning: The module is deprecated in version 0.21 and will be removed in version 0.23 since we've dropped support for Python 2.7. Please rely on the official version of six (https://pypi.org/project/six/).\n",
      "  \"(https://pypi.org/project/six/).\", FutureWarning)\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:144: FutureWarning: The sklearn.neighbors.base module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.neighbors. Anything that cannot be imported from sklearn.neighbors is now part of the private API.\n",
      "  warnings.warn(message, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "from imblearn.over_sampling import BorderlineSMOTE, SMOTE, ADASYN, SMOTENC, RandomOverSampler\n",
    "from imblearn.under_sampling import (RandomUnderSampler, \n",
    "                                    NearMiss, \n",
    "                                    InstanceHardnessThreshold,\n",
    "                                    CondensedNearestNeighbour,\n",
    "                                    EditedNearestNeighbours,\n",
    "                                    RepeatedEditedNearestNeighbours,\n",
    "                                    AllKNN,\n",
    "                                    NeighbourhoodCleaningRule,\n",
    "                                    OneSidedSelection,\n",
    "                                    TomekLinks)\n",
    "from imblearn.combine import SMOTEENN, SMOTETomek\n",
    "from imblearn.pipeline import make_pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "efoIi6euYMQs"
   },
   "source": [
    "Итак, в качестве инструментов для аугментации рассмотрим: under-sampling, over-sampling и их комбинацию."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DqhD2dodYwJl"
   },
   "source": [
    "**Under-sampling** уравновешивает данные за счет уменьшения размера  превалирующего класса.\n",
    "Этот метод разумно использовать, когда количество данных достаточно велико, иначе есть риск остаться и вовсе без обучающих примеров.\n",
    "\n",
    "Итак, логика действия довольно проста: мы просто случайным образом убираем лишние экземпляры из превалирующего класса.\n",
    "\n",
    "Так как в нашем примере лишь 7% всех твитов имеют негативную окраску, уравновешивание позитивного набора с этими 7-ю процентами вряд ли обеспечит хороший результат.\n",
    "\n",
    "Попробуем..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AQVfIvf0aTrp",
    "outputId": "a44e1060-6699-4747-b00c-16d8b720ffeb"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic regressio RUS, WordLevel TFIDF:  0.5184033177812338\n"
     ]
    }
   ],
   "source": [
    "rus = RandomUnderSampler(random_state=0, replacement=True)\n",
    "rus_xtrain_tfidf, rus_train_y = rus.fit_sample(xtrain_tfidf, train_y)\n",
    "accuracyrus = train_model(linear_model.LogisticRegression(random_state=0, solver='lbfgs',multi_class='multinomial'),rus_xtrain_tfidf, rus_train_y, xvalid_tfidf)\n",
    "print (\"Logistic regressio RUS, WordLevel TFIDF: \", accuracyrus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JnwqxfGeaa9R"
   },
   "source": [
    "Действительно, все стало только хуже.\n",
    "\n",
    "Попробуем другие алгоритмы **under-sampling**.\n",
    "\n",
    "Например, **NearMiss**. Данный алгоритм выбирает, какие экземпляры нужно оставить в превалирующем классе на основании некоторых эвристик. Существует три варианта данного алгоритма:\n",
    "\n",
    "**NearMiss-1** оставляет те экземпляры из превалирующего класса, для которых среднее расстояние до *k* ближайших соседей из миноритарного класса будет наименьшим.\n",
    "\n",
    "**NearMiss-2** оставляет те экземпляры из превалирующего класса, для которых среднее расстояние до *k* самых дальних соседей из миноритарного класса будет наименьшим.\n",
    "\n",
    "**NearMiss-3** состоит из двух шагов: сначала, для каждого экземпляра из миноритарного класса выбирается *k* ближайших соседей из превалирующего класса, затем, из большего класса выбираются те экземпляры, для которых среднее расстояние до *k* ближайших соседей максимальное.\n",
    "\n",
    "![](https://glemaitre.github.io/imbalanced-learn/_images/sphx_glr_plot_nearmiss_001.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QSlgmHmOd9tU",
    "outputId": "c252464b-3a48-48ef-b04f-5bb95d8e32f0"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic regression NearMiss(version= 1), WordLevel TFIDF:  0.3019441069258809\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic regression NearMiss(version= 2), WordLevel TFIDF:  0.5128205128205128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24.\n",
      "  warnings.warn(msg, category=FutureWarning)\n",
      "/usr/local/lib/python3.7/dist-packages/imblearn/under_sampling/_prototype_selection/_nearmiss.py:194: UserWarning: The number of the samples to be selected is larger than the number of samples available. The balancing ratio cannot be ensure and all samples will be returned.\n",
      "  warnings.warn('The number of the samples to be selected is larger'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic regression NearMiss(version= 3), WordLevel TFIDF:  0.3129129129129129\n"
     ]
    }
   ],
   "source": [
    "for sampler in (NearMiss(version=1),NearMiss(version=2),NearMiss(version=3)):\n",
    "    nm_xtrain_tfidf, nm_train_y = sampler.fit_sample(xtrain_tfidf, train_y)\n",
    "    accuracysm = train_model(linear_model.LogisticRegression(random_state=0, solver='lbfgs',multi_class='multinomial'),nm_xtrain_tfidf, nm_train_y, xvalid_tfidf)\n",
    "    print (\"Logistic regression NearMiss(version= {0}), WordLevel TFIDF: \".format(sampler.version), accuracysm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A9nNg2irexcc"
   },
   "source": [
    "**Edited Nearest Neighbor (ENN)**\n",
    "\n",
    "ENN удаляет из большего класса элемент, если класс его ближайшего соседа отличается от его собственного."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WFjd6PzjfWEU",
    "outputId": "3e13c72d-77e9-4ee7-e27d-5a73585847b8"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic regression {0}, WordLevel TFIDF:  0.5518072289156627\n"
     ]
    }
   ],
   "source": [
    "enn_xtrain_tfidf, enn_train_y = EditedNearestNeighbours().fit_sample(xtrain_tfidf, train_y)\n",
    "accuracy = train_model(linear_model.LogisticRegression(random_state=0, solver='lbfgs',multi_class='multinomial'),enn_xtrain_tfidf, enn_train_y, xvalid_tfidf)\n",
    "print (\"Logistic regression {0}, WordLevel TFIDF: \", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3VcHHV4jgZ50"
   },
   "source": [
    "Как вы поняли, при применении **Under-samplin**g техник новые данные не генерируются, в отличие от **Over-sampling**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2CHYj7jW7z5q"
   },
   "source": [
    "# Over-sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ODhcR4G672lq"
   },
   "source": [
    "Итак, когда данных недостаточно или количество экземпляров в миноритарном классе очень мало применяется **Over-sampling**. \n",
    "\n",
    "При применении этой техники балансировка данных происходит за счет увеличения количества экземпляров в миноритарном классе. Новые элементы генерируются за счет: повторения, бутстрэппинга, SMOTE (Synthetic Minority Over-Sampling Technique) или ADASYN (Adaptive synthetic sampling)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ea9aYY4mxsqo"
   },
   "source": [
    "**Random Over-sampling**: случайным образом дублируются некоторые элементы из миноритарного класса."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "t-6Ivm0ZOCOt",
    "outputId": "b8a90861-3ef1-4f1f-e460-0aaf5d37edea"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic regression ROS, WordLevel TFIDF:  0.6687354538401862\n"
     ]
    }
   ],
   "source": [
    "#Random Over Sampling\n",
    "ros = RandomOverSampler(random_state=777)\n",
    "ros_xtrain_tfidf, ros_train_y = ros.fit_sample(xtrain_tfidf, train_y)\n",
    "accuracyROS = train_model(linear_model.LogisticRegression(random_state=0, solver='lbfgs',multi_class='multinomial'),ros_xtrain_tfidf, ros_train_y, xvalid_tfidf)\n",
    "print (\"Logistic regression ROS, WordLevel TFIDF: \", accuracyROS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aXp1gpdDz5p5"
   },
   "source": [
    "**SMOTE Over-sampling**\n",
    "\n",
    "Алгоритм SMOTE основан на идее генерации некоторого количества искусственных примеров, которые были бы «похожи» на имеющиеся в миноритарном классе, но при этом не дублировали их.\n",
    "\n",
    "Для создания новой записи находят разность $d=X_b-X_a,$ где $ X_b, X_a -$ векторы признаков «соседних» примеров $a$ и $b$ из миноритарного класса. \n",
    "\n",
    "Их находят, используя алгоритм ближайшего соседа (*KNN*). В данном случае необходимо и достаточно для примера $b$ получить набор из $k$ соседей, из которого в дальнейшем будет выбрана запись $a$. Остальные шаги алгоритма *KNN* не требуются.\n",
    "\n",
    "Далее из $d$ путем умножения каждого его элемента на случайное число в интервале (0, 1) получают $\\hat{d}$. Вектор признаков нового примера вычисляется путем сложения $X_a$ и $\\hat{d}$. \n",
    "\n",
    "Алгоритм **SMOTE** позволяет задавать количество записей, которое необходимо искусственно сгенерировать. Степень сходства примеров $a$ и $b$ можно регулировать путем изменения значения $k$ (числа ближайших соседей).\n",
    "\n",
    "![](https://hsto.org/getpro/habr/post_images/c57/e7e/f4f/c57e7ef4f8711ad2eda881651a027867.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HWzqbIWsOEg1",
    "outputId": "53b6d551-384a-4189-e4c6-22689dba3eea"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic regression SMOTE, WordLevel TFIDF:  0.6593233674272228\n"
     ]
    }
   ],
   "source": [
    "sm = SMOTE(random_state=777, ratio = 1.0)\n",
    "sm_xtrain_tfidf, sm_train_y = sm.fit_sample(xtrain_tfidf, train_y)\n",
    "accuracySMOTE = train_model(linear_model.LogisticRegression(random_state=0, solver='lbfgs',multi_class='multinomial'),sm_xtrain_tfidf, sm_train_y, xvalid_tfidf)\n",
    "print (\"Logistic regression SMOTE, WordLevel TFIDF: \", accuracySMOTE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9gpJkvdb10Sg"
   },
   "source": [
    "Итак, по сравнению с **Random Over-sampling** разница небольшая.\n",
    "\n",
    "Проверьте результаты **Random Over-sampling** и **SMOTE Over-sampling** для реальных тестовых данных (*test_clean*)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7V-VoCAO7qw0"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0Jz0o7k82YoX"
   },
   "source": [
    "Следующий алгоритм **ASMO: Adaptive synthetic minority oversampling**.\n",
    "\n",
    "\n",
    "\n",
    "Сгенерировать искусственные записи в пределах отдельных кластеров на основе всех классов. Для каждого примера миноритарного класса находят m ближайших соседей, и на основе них (также как в SMOTE) создаются новые записи.\n",
    "\n",
    "1.   Если для каждого $i$-ого примера миноритарного класса из $k$ ближайших соседей $g$ ($g\\leq k$) принадлежит к мажоритарному, то набор данных считается «рассеянным». В этом случае используют алгоритм **ASMO**, иначе применяют **SMOTE** (как правило, $g$ задают равным 20).\n",
    "2.   Используя только примеры миноритарного класса, выделить несколько кластеров (например, алгоритмом $k$-means).\n",
    "3.   Сгенерировать искусственные записи в пределах отдельных кластеров на основе всех классов. Для каждого примера миноритарного класса находят m ближайших соседей, и на основе них (также как в **SMOTE**) создаются новые записи.\n",
    "\n",
    "![](https://encrypted-tbn0.gstatic.com/images?q=tbn%3AANd9GcQdTzjHBZ_9At5GIDRpF2AAw9hU1jzcVE5uwA&usqp=CAU)\n",
    "\n",
    "Такая модификация алгоритма **SMOTE** делает его более адаптивным к различным наборам данных с несбалансированными классами."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mClRDnFM1weo",
    "outputId": "d299f772-e119-42a4-b6e0-4f40a15520ce"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic regression ADASYN, WordLevel TFIDF:  0.6555639666919001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    }
   ],
   "source": [
    "ad = ADASYN(random_state=777, ratio = 1.0)\n",
    "ad_xtrain_tfidf, ad_train_y = ad.fit_sample(xtrain_tfidf, train_y)\n",
    "accuracyADASYN = train_model(linear_model.LogisticRegression(random_state=0, solver='lbfgs',multi_class='multinomial'),ad_xtrain_tfidf, ad_train_y, xvalid_tfidf)\n",
    "print (\"Logistic regression ADASYN, WordLevel TFIDF: \", accuracyADASYN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iHRcvIRP4fx7"
   },
   "source": [
    "И опять проверим на реальных тестовых примерах."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YTzMbjvnRyFD"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "br9FjH077ix0"
   },
   "source": [
    "# Комбинация **Under-** и **Over-sampling**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_-RCI42f42Hi"
   },
   "source": [
    "В *imblearn* реализованы две возможные комбинации:\n",
    "\n",
    "\n",
    "1.   **SMOTE** + **ENN**\n",
    "2.   **SMOTE** + **Tomek Link Removal** (Пара двух ближайших соседей, которые принадлежат разным классам называется *Tomek link*. Under-sampling заключается в удалении всех таких элементов из мажоритарного класса)\n",
    "\n",
    "Подробнее: https://imbalanced-learn.readthedocs.io/en/stable/api.html#module-imblearn.combine\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3pg1YBvT4-xP",
    "outputId": "fd5e0b9a-f1ae-47cb-9745-3cd348917748"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic regression SMOTEENN:  0.5139720558882237\n"
     ]
    }
   ],
   "source": [
    "se = SMOTEENN(random_state=42)\n",
    "se_xtrain_tfidf, se_train_y = se.fit_sample(xtrain_tfidf, train_y)\n",
    "accuracy = train_model(linear_model.LogisticRegression(random_state=0, solver='lbfgs',multi_class='multinomial'),se_xtrain_tfidf, se_train_y, xvalid_tfidf)\n",
    "print (\"Logistic regression SMOTEENN: \", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6Gd9dCEd5aDs"
   },
   "source": [
    "Первый метод сработал плохо. Оцените работу второго подхода."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bkZofkXo5RG2"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X52CbSUs3jkK"
   },
   "source": [
    "# Аугментация текстовых данных"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8YuQhjhhp0KM"
   },
   "source": [
    "Для аугментации тектсовых данных будем применять библиотеку [nlpaug](https://github.com/makcedward/nlpaug).\n",
    "\n",
    "Библиотека предоставляет функционал для различных типов аугментации текста, в том числе: добавление опечаток, замена слов синонимами, вставка дополнительных слов и т.д. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WjEpJ6Yz3r5D",
    "outputId": "423acdee-a711-402b-caac-7e43f2d72e75"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (1.19.5)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (2.23.0)\n",
      "Requirement already satisfied: nlpaug in /usr/local/lib/python3.7/dist-packages (1.1.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests) (2021.5.30)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests) (1.24.3)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests) (3.0.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests) (2.10)\n"
     ]
    }
   ],
   "source": [
    "!pip install numpy requests nlpaug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AuXPGhVT5JkI",
    "outputId": "5eca651b-62ef-4a35-eb36-53176edbd3c4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The quick brown fox jumps over the lazy dog .\n"
     ]
    }
   ],
   "source": [
    "text = 'The quick brown fox jumps over the lazy dog .'\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ihSt3nkBrKQ2"
   },
   "source": [
    "Аугментируем пример с помощью замены слов на основе векторной модели предсавления слов Glove.\n",
    "\n",
    "Загрузим и инициализируем модель:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ldMd_LBUbJVD"
   },
   "outputs": [],
   "source": [
    "from nlpaug.util.file.download import DownloadUtil\n",
    "\n",
    "\n",
    "DownloadUtil.download_glove(model_name='glove.6B', dest_dir='.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DbuG44538kFO"
   },
   "outputs": [],
   "source": [
    "import nlpaug.augmenter.word as naw\n",
    "\n",
    "aug = naw.WordEmbsAug(\n",
    "    model_type='glove', model_path='glove.6B.100d.txt',\n",
    "    action=\"substitute\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w3fAiWg6rju0"
   },
   "source": [
    "Аугментируем наш пример:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JqsycH7me1F5",
    "outputId": "c2e4eeae-b49d-4bd0-e776-038671905c8a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original:\n",
      "The quick brown fox jumps over the lazy dog .\n",
      "Augmented Text:\n",
      "The quick brown shown turns over the lazy monster.\n"
     ]
    }
   ],
   "source": [
    "augmented_text = aug.augment(text)\n",
    "print(\"Original:\")\n",
    "print(text)\n",
    "print(\"Augmented Text:\")\n",
    "print(augmented_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PLv4TNy6rphX"
   },
   "source": [
    "Попробуем аугментировать негативные твиты для классификатора. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1RtQJC8X64BW"
   },
   "outputs": [],
   "source": [
    "def get_negative_tweets(train_x, train_y):\n",
    "  x_neg_train = []\n",
    "  for tweet, label in zip(train_x, train_y):\n",
    "    if label == 1:\n",
    "      x_neg_train.append(tweet)\n",
    "  return x_neg_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wFbQsRzx74gC",
    "outputId": "31d7071c-156b-4168-bdf3-d7176799bf27"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['um whi a girl and not a boy or mayb just gender neutral teenag',\n",
       " 'clearli just humour attempt by liber media to undermin trump racebait won t work trump is not a',\n",
       " 'w prop jesu wa not black nor wa he white those who promot the hate biggotri need to be call vile scum',\n",
       " 'thi moron is lead the us down a path that ha so much potenti to damag and destroy the world',\n",
       " 'techjunkiejh the daili show unpack against blackwomen and asianmen on datingapp',\n",
       " 'you might be a libtard if libtard sjw liber polit',\n",
       " 'you might be a libtard if libtard sjw liber polit',\n",
       " 'allahsoil not all muslim hate america emirati in word',\n",
       " 'if they say their comment are just a joke or smth it will be just their opinion i don t think so it s a',\n",
       " 'riyadh is renown for some of the deadliest traffic in the world in word']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neg_train_x = get_negative_tweets(train_x, train_y)\n",
    "neg_train_x[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FDV3Xzht8F9G",
    "outputId": "00447728-5101-4a12-a4bf-9e8e2c05b961"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['um whi to her and not a boy or mayb just feminism extremely teenag',\n",
       " 'clearli just humour attempt by jahrbuch media up undermin trump racebait trophy t did trump actually not a',\n",
       " \"w prop jesu wa believe black unless wa he white those thought promot the hate biggotri n't to one call awful scum\",\n",
       " 'thi arka where lead the coming down a turn that ha so much potenti out damag and destroy the record',\n",
       " 'techjunkiejh another daili show individualize against blackwomen way asianmen on datingapp',\n",
       " 'tell might be a libtard if libtard js03 poupée polit',\n",
       " 'feel might be long libtard if libtard shn liber polit',\n",
       " 'allahsoil not it muslim remember america emirati in word',\n",
       " 'if they say their comment are just it joke or smth its time be just up opinion mind don t think so you price a',\n",
       " 'riyadh is admired for some an the deadliest traffic with even world in word']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neg_train_x_aug = aug.augment(neg_train_x)\n",
    "neg_train_x_aug[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "klAMj1LYtGCm"
   },
   "source": [
    "Добавим аугментированные данные к обучающей выборке:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Uuhelg3BhkZY"
   },
   "outputs": [],
   "source": [
    "train_x_aug = train_x.append(pd.Series(neg_train_x_aug))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9-iiMiuJtN81"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "neg_train_y_aug = [1] * len(neg_train_x_aug)\n",
    "train_y_aug = np.concatenate((train_y, neg_train_y_aug), axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "790P21a6tXYE"
   },
   "source": [
    "Перемешаем обучающие данные:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "14V3GHFBlnFY",
    "outputId": "c3302308-6f89-4166-9815-1813974dfc9b"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25640/25640 [00:56<00:00, 456.65it/s]\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from tqdm import tqdm\n",
    "\n",
    "indexes = list(range(0, len(train_x_aug)))\n",
    "random.shuffle(indexes)\n",
    "\n",
    "train_x_aug_shuffled = pd.Series([list(train_x_aug)[i] for i in tqdm(indexes)])\n",
    "train_y_aug_shuffled = train_y_aug[indexes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rhKtfS3fn63X"
   },
   "outputs": [],
   "source": [
    "#tweets = pd.concat([train_x1, valid_x])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VN1H_4IMujB8"
   },
   "source": [
    "Векторизуем полученные тексты:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rlgH2zwpgdaH"
   },
   "outputs": [],
   "source": [
    "tfidf_vect = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', max_features=100000)\n",
    "tfidf_vect.fit(pd.concat([train_x_aug_shuffled, valid_x]))\n",
    "xtrain_tfidf_aug =  tfidf_vect.transform(train_x_aug_shuffled)\n",
    "xvalid_tfidf_aug =  tfidf_vect.transform(valid_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-yYm3fbAunUp"
   },
   "source": [
    "Обучим и оценим модель:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tnbYZIGEkiKS",
    "outputId": "cba02701-0d99-4eec-e6ed-ae4d09b845e8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic regression with augmented texts, WordLevel TFIDF:  0.42416107382550333\n"
     ]
    }
   ],
   "source": [
    "accuracyTEXTAUG = train_model(linear_model.LogisticRegression(random_state=0, solver='lbfgs',multi_class='multinomial'),xtrain_tfidf_aug, train_y_aug_shuffled, xvalid_tfidf_aug)\n",
    "print (\"Logistic regression with augmented texts, WordLevel TFIDF: \", accuracyTEXTAUG)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "sem3_cnn_classification.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
